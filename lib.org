#+auto_tangle: t
* Data
:PROPERTIES:
:header-args: :eval no-export :tangle data.py :results silent
:END:
** Libraries
#+begin_src jupyter-python
import deeptrack as dt
import numpy as np
from itertools import count
import tensorflow as tf
#+end_src
** Create dataset
#+begin_src jupyter-python
def create_particle_dataset(image_size=64, sequence_length=8, batch_size=8):
    MIN_SIZE=.5e-6
    MAX_SIZE=1.5e-6
    MAX_VEL=10 # Maximum velocity. The higher the trickier!
    MAX_PARTICLES=3 # Max number of particles in each sequence. The higher the trickier!

    # Defining properties of the particles
    particle=dt.Sphere(intensity=lambda: 10+10*np.random.rand(),
                       radius=lambda: MIN_SIZE+np.random.rand()*(MAX_SIZE-MIN_SIZE),
                       position=lambda: image_size*np.random.rand(2),vel=lambda: MAX_VEL*np.random.rand(2),
                       position_unit="pixel")

    # Defining an update rule for the particle position
    def get_position(previous_value,vel):
        newv=previous_value+vel
        for i in range(2):
            if newv[i]>63:
                newv[i]=63-np.abs(newv[i]-63)
                vel[i]=-vel[i]
            elif newv[i]<0:
                newv[i]=np.abs(newv[i])
                vel[i]=-vel[i]
        return newv

    particle=dt.Sequential(particle,position=get_position)

    # Defining properties of the microscope
    optics=dt.Fluorescence(NA=1,output_region= (0, 0,image_size, image_size),
                           magnification=10,
                           resolution=(1e-6, 1e-6),
                           wavelength=633e-9)

    # Combining everything into a dataset. Note that the sequences are flipped in
    # different directions, so that each unique sequence defines in fact 8 sequences
    # flipped in different directions, to speed up data generation
    dataset=dt.FlipUD(dt.FlipDiagonal(dt.FlipLR(dt.Sequence(optics(particle**(lambda: 1+np.random.randint(MAX_PARTICLES))),sequence_length=sequence_length))))

    return dataset
#+end_src

** Create frame dataset
#+name: auto-dataset
#+begin_src jupyter-python
def create_autoencoder_generator(image_size=64, batch_size=8):
    dataset = create_particle_dataset(image_size, 1, batch_size)
    generator = dt.generators.Generator()
    gen = generator.generate(
        dataset,
        lambda x: x,
        batch_size=batch_size,
    )
    return gen
#+end_src

** Create sequence dataset
#+name: sequence-dataset
#+begin_src jupyter-python
def create_sequence_generator(image_size=64, batch_size=8, prior_length=8, truth_length=1):
    dataset = create_particle_dataset(image_size, prior_length + truth_length, batch_size)
    generator = dt.generators.Generator()
    gen = generator.generate(
        dataset,
        batch_function = lambda x: x[:prior_length],
        label_function = lambda x: x[prior_length:],
        batch_size=batch_size,
        ndim=5,
    )

    return gen
#+end_src

* Callbacks
:PROPERTIES:
:header-args: :eval no-export :tangle callbacks.py :results silent
:END:
** Libraries
#+begin_src jupyter-python
from tensorflow import keras
import tensorflow as tf
#+end_src

** Image writer
#+begin_src jupyter-python
class LogImageCallback(keras.callbacks.Callback):
    def __init__(self, log_dir, reference_images):
        self.image_writer = tf.summary.create_file_writer(log_dir + '/images')
        self.num_images = reference_images.shape[0]
        self.references = reference_images

    def on_train_begin(self, logs=None):
        with self.image_writer.as_default():
            tf.summary.image("auto/Original", self.references / tf.reduce_max(self.references), max_outputs=self.num_images, step=0)

    def on_epoch_begin(self, epoch, logs=None):
        reconstructed = self.model.predict(self.references)
        with self.image_writer.as_default():
            tf.summary.image("auto/Reconstructed", reconstructed / tf.reduce_max(self.references), max_outputs=self.num_images, step=epoch)

#+end_src
** Sequence writer
#+begin_src jupyter-python
class LogImageSequenceCallback(keras.callbacks.Callback):
    def __init__(self, log_dir, prior, truth):
        self.image_writer = tf.summary.create_file_writer(log_dir + '/images')
        self.prior_length = prior.shape[0]
        self.truth_length = truth.shape[0]
        self.prior = prior
        self.truth = truth

    def on_train_begin(self, logs=None):
        with self.image_writer.as_default():
            tf.summary.image("seq/Prior", self.prior, max_outputs=self.prior_length, step=0)
            tf.summary.image("seq/Truth", self.truth, max_outputs=self.truth_length, step=0)

    def on_epoch_begin(self, epoch, logs=None):
        encoded_x = tf.expand_dims(self.model.autoencoder.encoder(self.prior), 0)
        pred = self.model.predict(encoded_x)
        if self.truth_length != 1: pred = tf.squeeze(pred)
        decoded = self.model.autoencoder.decoder(pred)
        with self.image_writer.as_default():
            tf.summary.image("seq/Prediction", decoded, max_outputs=self.truth_length, step=epoch)
#+end_src
* Models
:PROPERTIES:
:header-args: :eval no-export :tangle models.py :results silent
:END:
** Libraries
#+begin_src jupyter-python
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.layers as layers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv1D, Conv2D, MaxPooling2D, Dense, Flatten, Reshape, UpSampling2D, Concatenate, Dropout
import numpy as np
#+end_src
** Autoencoder
#+name: auto-decoder
#+begin_src jupyter-python
class ConvolutionDecoder(Model):
    def __init__(self, image_size, code_dim, depth):
        super(ConvolutionDecoder, self).__init__()
        self.code_dim = code_dim
        self.image_size = image_size
        self.depth = depth

    def build(self, input_shape):
        self.dense_1 = Dense(4*4*16)
        self.reshape = Reshape((4,4,16))
        self.convs = [
            Conv2D((16*2**(self.depth)) // 2**(i),
                   kernel_size=3, strides=1, padding='same', activation='relu')
            for i in range(self.depth)
        ]
        self.upsamples = [UpSampling2D((2,2)) for _ in range(self.depth)]
        self.final_conv = Conv2D(1, kernel_size=3, strides=1, padding='same')

    def call(self, inputs):
        out = self.dense_1(inputs)
        out = self.reshape(out)

        for i in range(self.depth):
            out = self.convs[i](out)
            out = self.upsamples[i](out)

        out = self.final_conv(out)
        return out
#+end_src

#+name: auto-encoder
#+begin_src jupyter-python
class ConvolutionEncoder(Model):
    def __init__(self, image_size, code_dim, depth):
        super(ConvolutionEncoder, self).__init__()
        self.code_dim = code_dim
        self.image_size = image_size
        self.depth = depth

    def build(self, input_shape):
        self.convs = [
            Conv2D(16 * 2**(i),
                   3, strides=2, activation='relu')
            for i in range(self.depth)
        ]
        self.flat = Flatten()
        self.dense_1 = Dense(self.code_dim)

    def call(self, inputs):
        out = inputs
        for l in self.convs:
            out = l(out)

        out = self.flat(out)
        return self.dense_1(out)

#+end_src

#+name: auto-complete
#+begin_src jupyter-python
class ConvolutionalAutoencoder(Model):
    def __init__(self, image_size, code_dim, depth):
        super(ConvolutionalAutoencoder, self).__init__()
        self.image_size = image_size
        self.code_dim = code_dim
        self.depth = depth

    def build(self, input_shape):
        self.encoder = ConvolutionEncoder(self.image_size, self.code_dim, self.depth)
        self.decoder = ConvolutionDecoder(self.image_size, self.code_dim, self.depth)

    def get_config(self):
        return {
            "image_size": self.image_size,
            "code_dim": self.code_dim,
            "depth": self.depth,
        }

    def call(self, inputs):
        encoding = self.encoder(inputs)
        encoding = layers.GaussianNoise(0.1)(encoding)
        decoding = self.decoder(encoding)
        return decoding
#+end_src

** Transformer
#+begin_src jupyter-python
class Time2Vector(Layer):
    def __init__(self, seq_len, **kwargs):
        super(Time2Vector, self).__init__()
        self.seq_len = seq_len

    def build(self, input_shape):
        self.weights_linear = self.add_weight(name='weight_linear',
                                            shape=(int(self.seq_len),),
                                            initializer='uniform',
                                            trainable=True)

        self.bias_linear = self.add_weight(name='bias_linear',
                                        shape=(int(self.seq_len),),
                                        initializer='uniform',
                                        trainable=True)

        self.weights_periodic = self.add_weight(name='weight_periodic',
                                                shape=(int(self.seq_len),),
                                                initializer='uniform',
                                                trainable=True)

        self.bias_periodic = self.add_weight(name='bias_periodic',
                                            shape=(int(self.seq_len),),
                                            initializer='uniform',
                                            trainable=True)

    def call(self, x):
        x = tf.math.reduce_mean(x, axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)
        time_linear = self.weights_linear * x + self.bias_linear
        time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)

        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)
        time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)
        return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)
#+end_src

#+begin_src jupyter-python
# Attention layer
class SingleAttention(Layer):
    def __init__(self, d_k, d_v):
        super(SingleAttention, self).__init__()
        self.d_k = d_k
        self.d_v = d_v

    def build(self, input_shape):
        self.query = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')
        self.key = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')
        self.value = Dense(self.d_v, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')

    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)
        q = self.query(inputs[0])
        k = self.key(inputs[1])

        attn_weights = tf.matmul(q, k, transpose_b=True)
        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)
        attn_weights = tf.nn.softmax(attn_weights, axis=-1)

        v = self.value(inputs[2])
        attn_out = tf.matmul(attn_weights, v)
        return attn_out

#+end_src
#+begin_src jupyter-python
# Multihead attention
class MultiAttention(Layer):
    def __init__(self, d_k, d_v, n_heads,filt_dim):
        super(MultiAttention, self).__init__()
        self.d_k = d_k
        self.d_v = d_v
        self.n_heads = n_heads
        self.filt_dim=filt_dim
        self.attn_heads = list()

    def build(self, input_shape):
        for n in range(self.n_heads):
            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))
            self.linear = Dense(self.filt_dim, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')

    def call(self, inputs):
        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]
        concat_attn = tf.concat(attn, axis=-1)
        multi_linear = self.linear(concat_attn)
        return multi_linear
#+end_src

#+begin_src jupyter-python
# Combining everything into a Transformer encoder
class TransformerEncoder(Layer):
    def __init__(self, d_k, d_v, n_heads, ff_dim, filt_dim, dropout=0.1, **kwargs):
        super(TransformerEncoder, self).__init__()
        self.d_k = d_k
        self.d_v = d_v
        self.n_heads = n_heads
        self.ff_dim = ff_dim
        self.filt_dim=filt_dim
        self.attn_heads = list()
        self.dropout_rate = dropout

    def build(self, input_shape):
        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads,self.filt_dim)
        self.attn_dropout = layers.Dropout(self.dropout_rate)
        self.attn_normalize = layers.LayerNormalization(input_shape=input_shape, epsilon=1e-6)

        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')
        self.ff_conv1D_2 = Conv1D(filters=self.filt_dim, kernel_size=1) # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7
        self.ff_dropout = layers.Dropout(self.dropout_rate)
        self.ff_normalize = layers.LayerNormalization(input_shape=input_shape, epsilon=1e-6)

    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)
        attn_layer = self.attn_multi(inputs)
        attn_layer = self.attn_dropout(attn_layer)
        attn_layer = self.attn_normalize(inputs[0] + attn_layer)

        ff_layer = self.ff_conv1D_1(attn_layer)
        ff_layer = self.ff_conv1D_2(ff_layer)
        ff_layer = self.ff_dropout(ff_layer)
        ff_layer = self.ff_normalize(inputs[0] + ff_layer)
        return ff_layer

#+end_src

** Particle predictor

#+name: transformer
#+begin_src jupyter-python
class ParticlePredictor(Model):
    def __init__(self, autoencoder, code_dim, prior_len, truth_len, d_k, d_v, n_heads=1, ff_dim=16, filt_dim=4):
        super(ParticlePredictor, self).__init__()
        self.autoencoder = autoencoder
        self.code_dim = code_dim
        self.d_k = d_k
        self.d_v = d_v
        self.prior_len = prior_len
        self.truth_len = truth_len
        self.filt_dim=filt_dim
        self.n_heads = n_heads
        self.ff_dim = ff_dim

        self.time_embedding = Time2Vector(self.prior_len)
        self.attn1 = TransformerEncoder(self.d_k, self.d_v, self.n_heads, self.ff_dim, self.filt_dim)
        self.bnorm = layers.BatchNormalization()
        self.attn2 = TransformerEncoder(self.d_k, self.d_v, self.n_heads, self.ff_dim, self.filt_dim)
        self.attn3 = TransformerEncoder(self.d_k, self.d_v, self.n_heads, self.ff_dim, self.filt_dim)
        # self.conv1d_1 = Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')
        self.dense1 = Dense(512, activation='relu')
        self.dense2 = Dense(self.code_dim, activation='linear')

    def build(self, input_shape):
        super(ParticlePredictor, self).build(input_shape)

    @tf.function
    def call(self, inputs):
        time_embeddings = self.time_embedding(inputs)
        x = layers.Concatenate(axis=-1)([time_embeddings, inputs])
        x = self.attn1([x,x,x])
        x = self.attn2([x,x,x])
        x = self.attn3([x,x,x])
        x = layers.Flatten()(x)
        x = self.dense1(x)
        x = self.dense2(x)
        return x

    @tf.function
    def train_step(self, images):
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = images
        encoded_x = tf.map_fn(self.autoencoder.encoder, x)
        encoded_y = tf.map_fn(self.autoencoder.encoder, y)[:,0,:]

        with tf.GradientTape() as tape:
            y_pred = self(encoded_x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            # loss = self.compiled_loss(encoded_y, y_pred, regularization_losses=self.losses)
            loss = self.compiled_loss(encoded_y, y_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(encoded_y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

#+end_src

#+name: lstm-model
#+begin_src jupyter-python
class ParticlePredictorLSTM(Model):
    def __init__(self, autoencoder, code_dim, prior_len, truth_len, filt_dim=4):
        super(ParticlePredictorLSTM, self).__init__()
        self.autoencoder = autoencoder
        self.code_dim = code_dim
        self.prior_len = prior_len
        self.truth_len = truth_len
        self.filt_dim=filt_dim

        self.time_embedding = Time2Vector(self.prior_len)
        self.lstm1 = layers.LSTM(512, return_sequences=True)
        self.lstm2 = layers.LSTM(512, return_sequences=False)
        self.dense1 = Dense(self.code_dim)
        self.dense2 = Dense(256, activation='relu')
        self.dense3 = Dense(self.code_dim)
        self.dense4 = Dense(256, activation='relu')
        self.dense5 = Dense(self.code_dim)

    def build(self, input_shape):
        super(ParticlePredictorLSTM, self).build(input_shape)

    @tf.function
    def call(self, inputs):
        x = self.lstm1(inputs)
        x = self.lstm2(x)
        res = self.dense1(x)
        x = self.dense2(x)
        res += self.dense3(x)
        x = self.dense4(x)
        res += self.dense5(x)
        return res

    @tf.function
    def train_step(self, images):
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = images
        encoded_x = tf.map_fn(self.autoencoder.encoder, x)
        encoded_y = tf.map_fn(self.autoencoder.encoder, y)[:,0,:]

        with tf.GradientTape() as tape:
            y_pred = self(encoded_x, training=True)  # Forward pass
            # Compute the loss value
            # (the loss function is configured in `compile()`)
            # loss = self.compiled_loss(encoded_y, y_pred, regularization_losses=self.losses)
            loss = self.compiled_loss(encoded_y, y_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(encoded_y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

#+end_src
